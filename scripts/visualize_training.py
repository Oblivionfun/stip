#!/usr/bin/env python3
"""
å¯è§†åŒ–è®­ç»ƒå†å²ï¼ˆä»checkpointçš„trainer_state.jsonï¼‰
"""

import json
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path
import argparse


def load_training_history(checkpoint_path):
    """ä»checkpointåŠ è½½è®­ç»ƒå†å²"""
    trainer_state_file = Path(checkpoint_path) / "trainer_state.json"

    if not trainer_state_file.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° {trainer_state_file}")

    with open(trainer_state_file, 'r') as f:
        data = json.load(f)

    return data['log_history']


def plot_training_curves(log_history, output_path=None):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""

    # æå–æ•°æ®
    train_steps = []
    train_losses = []
    learning_rates = []
    grad_norms = []

    eval_steps = []
    eval_losses = []

    for entry in log_history:
        if 'loss' in entry and 'step' in entry:
            # è®­ç»ƒloss
            train_steps.append(entry['step'])
            train_losses.append(entry['loss'])

            if 'learning_rate' in entry:
                learning_rates.append(entry['learning_rate'])

            if 'grad_norm' in entry:
                grad_norms.append(entry['grad_norm'])

        elif 'eval_loss' in entry and 'step' in entry:
            # è¯„ä¼°loss
            eval_steps.append(entry['step'])
            eval_losses.append(entry['eval_loss'])

    # åˆ›å»ºå›¾è¡¨
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('Training Progress', fontsize=16, fontweight='bold')

    # 1. è®­ç»ƒLoss
    ax1 = axes[0, 0]
    ax1.plot(train_steps, train_losses, 'b-', linewidth=2, label='Train Loss')
    if eval_losses:
        ax1.plot(eval_steps, eval_losses, 'r--', linewidth=2, label='Eval Loss')
    ax1.set_xlabel('Steps')
    ax1.set_ylabel('Loss')
    ax1.set_title('Training & Evaluation Loss')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # 2. å­¦ä¹ ç‡
    ax2 = axes[0, 1]
    if learning_rates:
        ax2.plot(train_steps[:len(learning_rates)], learning_rates, 'g-', linewidth=2)
        ax2.set_xlabel('Steps')
        ax2.set_ylabel('Learning Rate')
        ax2.set_title('Learning Rate Schedule')
        ax2.grid(True, alpha=0.3)
        ax2.ticklabel_format(style='scientific', axis='y', scilimits=(0,0))

    # 3. æ¢¯åº¦èŒƒæ•°
    ax3 = axes[1, 0]
    if grad_norms:
        ax3.plot(train_steps[:len(grad_norms)], grad_norms, 'orange', linewidth=2)
        ax3.set_xlabel('Steps')
        ax3.set_ylabel('Gradient Norm')
        ax3.set_title('Gradient Norm')
        ax3.grid(True, alpha=0.3)

    # 4. Lossè¶‹åŠ¿ï¼ˆæ»‘åŠ¨å¹³å‡ï¼‰
    ax4 = axes[1, 1]
    if len(train_losses) > 10:
        window_size = min(50, len(train_losses) // 10)
        smoothed_loss = np.convolve(train_losses, np.ones(window_size)/window_size, mode='valid')
        smoothed_steps = train_steps[window_size-1:]
        ax4.plot(train_steps, train_losses, 'b-', alpha=0.3, label='Raw')
        ax4.plot(smoothed_steps, smoothed_loss, 'b-', linewidth=2, label=f'Smoothed (window={window_size})')
        ax4.set_xlabel('Steps')
        ax4.set_ylabel('Loss')
        ax4.set_title('Loss Trend (Smoothed)')
        ax4.legend()
        ax4.grid(True, alpha=0.3)

    plt.tight_layout()

    if output_path:
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"âœ“ è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ°: {output_path}")

    return fig


def print_statistics(log_history):
    """æ‰“å°è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
    train_losses = [entry['loss'] for entry in log_history if 'loss' in entry]

    if not train_losses:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°è®­ç»ƒlossæ•°æ®")
        return

    print("\n" + "=" * 80)
    print("è®­ç»ƒç»Ÿè®¡")
    print("=" * 80)

    print(f"\nğŸ“Š Lossç»Ÿè®¡:")
    print(f"  â€¢ åˆå§‹Loss: {train_losses[0]:.4f}")
    print(f"  â€¢ æœ€ç»ˆLoss: {train_losses[-1]:.4f}")
    print(f"  â€¢ æœ€ä½Loss: {min(train_losses):.4f}")
    print(f"  â€¢ Lossä¸‹é™: {train_losses[0] - train_losses[-1]:.4f} ({(1 - train_losses[-1]/train_losses[0])*100:.1f}%)")

    # æœ€è¿‘Næ­¥çš„å¹³å‡loss
    recent_n = min(100, len(train_losses) // 10)
    recent_avg = np.mean(train_losses[-recent_n:])
    print(f"  â€¢ æœ€è¿‘{recent_n}æ­¥å¹³å‡Loss: {recent_avg:.4f}")

    # è®­ç»ƒè¿›åº¦
    total_steps = log_history[-1].get('step', 0)
    epochs = log_history[-1].get('epoch', 0)

    print(f"\nğŸ“ˆ è®­ç»ƒè¿›åº¦:")
    print(f"  â€¢ æ€»æ­¥æ•°: {total_steps}")
    print(f"  â€¢ è®­ç»ƒè½®æ•°: {epochs:.2f} epochs")

    # è¯„ä¼°ç»“æœ
    eval_entries = [entry for entry in log_history if 'eval_loss' in entry]
    if eval_entries:
        print(f"\nğŸ¯ è¯„ä¼°ç»“æœ:")
        print(f"  â€¢ è¯„ä¼°æ¬¡æ•°: {len(eval_entries)}")
        eval_losses = [entry['eval_loss'] for entry in eval_entries]
        print(f"  â€¢ æœ€ä½³Eval Loss: {min(eval_losses):.4f}")
        print(f"  â€¢ æœ€æ–°Eval Loss: {eval_losses[-1]:.4f}")

    print("\n" + "=" * 80)


def main():
    parser = argparse.ArgumentParser(description="å¯è§†åŒ–è®­ç»ƒå†å²")
    parser.add_argument(
        '--checkpoint',
        type=str,
        default='checkpoints/sft_model/checkpoint-2000',
        help='Checkpointè·¯å¾„ï¼ˆé»˜è®¤ï¼šcheckpoint-2000ï¼‰'
    )
    parser.add_argument(
        '--output',
        type=str,
        default='outputs/training_curves.png',
        help='è¾“å‡ºå›¾ç‰‡è·¯å¾„'
    )
    parser.add_argument(
        '--show',
        action='store_true',
        help='æ˜¾ç¤ºå›¾è¡¨ï¼ˆéœ€è¦GUIç¯å¢ƒï¼‰'
    )

    args = parser.parse_args()

    print("=" * 80)
    print(f"åŠ è½½è®­ç»ƒå†å²: {args.checkpoint}")
    print("=" * 80)

    try:
        # åŠ è½½è®­ç»ƒå†å²
        log_history = load_training_history(args.checkpoint)

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print_statistics(log_history)

        # ç»˜åˆ¶æ›²çº¿
        fig = plot_training_curves(log_history, args.output)

        if args.show:
            plt.show()
        else:
            plt.close(fig)

        print(f"\nâœ“ å®Œæˆï¼")

    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
