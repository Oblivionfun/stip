#!/usr/bin/env python3
"""
æŸ¥çœ‹æ¨¡å‹é¢„æµ‹æ ·ä¾‹
ç›´æ¥åŠ è½½éªŒè¯æ•°æ®å¹¶ç”¨æŒ‡å®šæ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼Œå±•ç¤ºè¯¦ç»†è¾“å‡º
"""

import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from pathlib import Path
import argparse


def load_model(model_path):
    """åŠ è½½æ¨¡å‹"""
    print(f"Loading model from {model_path}...")
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        device_map="auto",
        torch_dtype=torch.bfloat16,
        trust_remote_code=True
    )
    model.eval()
    print("âœ“ Model loaded\n")
    return tokenizer, model


def predict_single(prompt, tokenizer, model):
    """é¢„æµ‹å•ä¸ªæ ·æœ¬"""
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=512,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id
        )

    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # æå–responseéƒ¨åˆ†
    if prompt in generated:
        response = generated[len(prompt):].strip()
    else:
        response = generated

    return response


def main():
    parser = argparse.ArgumentParser(description="æŸ¥çœ‹æ¨¡å‹é¢„æµ‹æ ·ä¾‹")
    parser.add_argument(
        "--model-path",
        type=str,
        default="model/models",
        help="æ¨¡å‹è·¯å¾„ (é»˜è®¤: model/models)"
    )
    parser.add_argument(
        "--num-samples",
        type=int,
        default=3,
        help="æ˜¾ç¤ºæ ·æœ¬æ•°é‡ (é»˜è®¤: 3)"
    )
    parser.add_argument(
        "--sample-id",
        type=str,
        default=None,
        help="æŒ‡å®šæ ·æœ¬ID (å¯é€‰)"
    )

    args = parser.parse_args()

    # åŠ è½½éªŒè¯æ•°æ®
    val_file = "outputs/validation_samples.jsonl"
    print(f"Loading validation samples from {val_file}...")
    samples = []
    with open(val_file, 'r', encoding='utf-8') as f:
        for line in f:
            samples.append(json.loads(line.strip()))
    print(f"âœ“ Loaded {len(samples)} samples\n")

    # å¦‚æœæŒ‡å®šäº†sample_id
    if args.sample_id:
        samples = [s for s in samples if s['id'] == args.sample_id]
        if not samples:
            print(f"âŒ æœªæ‰¾åˆ°IDä¸º {args.sample_id} çš„æ ·æœ¬")
            return
    else:
        samples = samples[:args.num_samples]

    # åŠ è½½æ¨¡å‹
    tokenizer, model = load_model(args.model_path)

    # é¢„æµ‹å¹¶æ˜¾ç¤º
    print("=" * 80)
    print("é¢„æµ‹ç»“æœ")
    print("=" * 80)

    for i, sample in enumerate(samples, 1):
        print(f"\n{'='*80}")
        print(f"æ ·æœ¬ {i}/{len(samples)}: {sample['id']}")
        print(f"{'='*80}")

        # æ˜¾ç¤ºprompt (æˆªæ–­)
        prompt_preview = sample['prompt'][:300].replace('\n', ' ')
        print(f"\nğŸ“ Prompt (å‰300å­—ç¬¦):")
        print(f"   {prompt_preview}...")

        # Ground Truth
        print(f"\nâœ… Ground Truth:")
        gt_response = sample['response']
        print(f"   Thinking: {gt_response.get('thinking', 'N/A')[:100]}...")
        print(f"   Reflection: {gt_response.get('reflection', 'N/A')[:100]}...")
        print(f"   Plan: {gt_response.get('plan', 'N/A')}")

        # æ¨¡å‹é¢„æµ‹
        print(f"\nğŸ¤– æ¨¡å‹é¢„æµ‹:")
        print(f"   æ­£åœ¨ç”Ÿæˆ...")
        prediction = predict_single(sample['prompt'], tokenizer, model)
        print(f"\n   åŸå§‹è¾“å‡º:")
        print(f"   {prediction[:500]}")

        # å°è¯•è§£æJSON
        try:
            # æŸ¥æ‰¾JSONéƒ¨åˆ†
            if '{' in prediction and '}' in prediction:
                json_start = prediction.find('{')
                json_end = prediction.rfind('}') + 1
                json_str = prediction[json_start:json_end]
                pred_json = json.loads(json_str)

                print(f"\n   è§£æåçš„JSON:")
                print(f"   Thinking: {pred_json.get('thinking', 'N/A')[:100]}...")
                print(f"   Reflection: {pred_json.get('reflection', 'N/A')[:100]}...")
                print(f"   Plan: {pred_json.get('plan', 'N/A')}")

                # å¯¹æ¯”
                gt_plan = gt_response.get('plan', '')
                pred_plan = pred_json.get('plan', '')
                match = 'âœ… åŒ¹é…' if gt_plan == pred_plan else 'âŒ ä¸åŒ¹é…'
                print(f"\n   å†³ç­–å¯¹æ¯”: {match}")
                print(f"     Ground Truth Plan: {gt_plan}")
                print(f"     Predicted Plan:    {pred_plan}")
            else:
                print(f"   âš ï¸  è¾“å‡ºä¸åŒ…å«æœ‰æ•ˆçš„JSONæ ¼å¼")
        except Exception as e:
            print(f"   âŒ JSONè§£æå¤±è´¥: {e}")

        print(f"\n{'='*80}\n")

    print("\nâœ“ é¢„æµ‹å®Œæˆ")


if __name__ == "__main__":
    main()
