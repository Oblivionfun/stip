#!/usr/bin/env python3
"""
è¯„ä¼°ç»“æœåˆ†æè„šæœ¬
ç”¨äºæŸ¥çœ‹å’Œå¯¹æ¯”baseline vs å¾®è°ƒæ¨¡å‹çš„æ€§èƒ½
"""

import json
import sys
from pathlib import Path


def load_results(filepath):
    """åŠ è½½è¯„ä¼°ç»“æœ"""
    with open(filepath, 'r', encoding='utf-8') as f:
        return json.load(f)


def print_metrics(results, title="è¯„ä¼°ç»“æœ"):
    """æ‰“å°è¯„ä¼°æŒ‡æ ‡"""
    print("\n" + "=" * 80)
    print(f"{title}")
    print("=" * 80)

    metrics = results['decision_metrics']

    print(f"\nğŸ“Š å†³ç­–å‡†ç¡®ç‡æŒ‡æ ‡:")
    print(f"  â€¢ è·¯å¾„é€‰æ‹©å‡†ç¡®ç‡: {metrics['route_selection_accuracy']*100:.2f}%")
    print(f"  â€¢ æ”¹é“å†³ç­–F1åˆ†æ•°: {metrics['reroute_f1']:.4f}")
    print(f"    - Precision: {metrics['reroute_precision']:.4f}")
    print(f"    - Recall: {metrics['reroute_recall']:.4f}")

    print(f"\nğŸ“ è¯­è¨€æ¨¡å‹æŒ‡æ ‡:")
    print(f"  â€¢ Perplexity: {results['perplexity']:.2f}")

    print(f"\nğŸ“ˆ è¯¦ç»†ç»Ÿè®¡:")
    print(f"  â€¢ æ€»æ ·æœ¬æ•°: {metrics['total_samples']}")
    print(f"  â€¢ æ­£ç¡®è·¯å¾„é€‰æ‹©: {metrics['correct_route']}")
    print(f"  â€¢ æ”¹é“TP (æ­£ç¡®æ”¹é“): {metrics['reroute_tp']}")
    print(f"  â€¢ æ”¹é“FP (é”™è¯¯æ”¹é“): {metrics['reroute_fp']}")
    print(f"  â€¢ æ”¹é“FN (åº”æ”¹æœªæ”¹): {metrics['reroute_fn']}")

    # å¦‚æœæœ‰é¢„æµ‹æ ·ä¾‹
    if 'predictions' in results and len(results['predictions']) > 0:
        print(f"\nğŸ” é¢„æµ‹æ ·ä¾‹ (å‰3ä¸ª):")
        for i, pred in enumerate(results['predictions'][:3], 1):
            print(f"\n  [{i}] é¢„æµ‹:")
            print(f"      {json.dumps(pred, ensure_ascii=False, indent=6)}")

    print("\n" + "=" * 80)


def compare_results(baseline_path, finetuned_path):
    """å¯¹æ¯”baselineå’Œå¾®è°ƒåçš„ç»“æœ"""
    baseline = load_results(baseline_path)
    finetuned = load_results(finetuned_path)

    print("\n" + "=" * 80)
    print("ğŸ“Š æ€§èƒ½å¯¹æ¯”ï¼šBaseline vs å¾®è°ƒå")
    print("=" * 80)

    b_metrics = baseline['decision_metrics']
    f_metrics = finetuned['decision_metrics']

    # è·¯å¾„é€‰æ‹©å‡†ç¡®ç‡
    b_acc = b_metrics['route_selection_accuracy'] * 100
    f_acc = f_metrics['route_selection_accuracy'] * 100
    acc_improvement = f_acc - b_acc

    print(f"\nğŸ¯ è·¯å¾„é€‰æ‹©å‡†ç¡®ç‡:")
    print(f"  Baseline:    {b_acc:6.2f}%")
    print(f"  å¾®è°ƒå:      {f_acc:6.2f}%")
    print(f"  æå‡:        {acc_improvement:+6.2f}% {'âœ…' if acc_improvement > 0 else 'âŒ'}")

    # æ”¹é“F1
    b_f1 = b_metrics['reroute_f1']
    f_f1 = f_metrics['reroute_f1']
    f1_improvement = f_f1 - b_f1

    print(f"\nğŸ”„ æ”¹é“F1åˆ†æ•°:")
    print(f"  Baseline:    {b_f1:.4f}")
    print(f"  å¾®è°ƒå:      {f_f1:.4f}")
    print(f"  æå‡:        {f1_improvement:+.4f} {'âœ…' if f1_improvement > 0 else 'âŒ'}")

    # Perplexity
    b_ppl = baseline['perplexity']
    f_ppl = finetuned['perplexity']
    ppl_change = f_ppl - b_ppl

    print(f"\nğŸ“ Perplexity (è¶Šä½è¶Šå¥½):")
    print(f"  Baseline:    {b_ppl:.2f}")
    print(f"  å¾®è°ƒå:      {f_ppl:.2f}")
    print(f"  å˜åŒ–:        {ppl_change:+.2f} {'âœ…' if ppl_change < 0 else 'âŒ'}")

    # ç›®æ ‡è¾¾æˆæƒ…å†µ
    print(f"\nğŸ¯ ç›®æ ‡è¾¾æˆæƒ…å†µ:")
    print(f"  è·¯å¾„å‡†ç¡®ç‡ â‰¥75%:  {'âœ… è¾¾æˆ' if f_acc >= 75 else 'âŒ æœªè¾¾æˆ'} ({f_acc:.1f}%)")
    print(f"  æ”¹é“F1 â‰¥0.80:      {'âœ… è¾¾æˆ' if f_f1 >= 0.80 else 'âŒ æœªè¾¾æˆ'} ({f_f1:.3f})")
    print(f"  Perplexity <10:    {'âœ… è¾¾æˆ' if f_ppl < 10 else 'âŒ æœªè¾¾æˆ'} ({f_ppl:.2f})")

    print("\n" + "=" * 80)


def main():
    if len(sys.argv) == 1:
        # åªæŸ¥çœ‹baseline
        baseline_file = "outputs/baseline_results.json"
        if Path(baseline_file).exists():
            results = load_results(baseline_file)
            print_metrics(results, "Baseline è¯„ä¼°ç»“æœ (æœªå¾®è°ƒ)")
        else:
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {baseline_file}")
            print("\nä½¿ç”¨æ–¹æ³•:")
            print("  python analyze_evaluation.py                    # æŸ¥çœ‹baseline")
            print("  python analyze_evaluation.py <result_file>      # æŸ¥çœ‹æŒ‡å®šç»“æœ")
            print("  python analyze_evaluation.py compare            # å¯¹æ¯”baselineå’Œå¾®è°ƒå")

    elif len(sys.argv) == 2:
        arg = sys.argv[1]

        if arg == "compare":
            # å¯¹æ¯”æ¨¡å¼
            baseline_file = "outputs/baseline_results.json"
            finetuned_file = "outputs/finetuned_results.json"

            if not Path(baseline_file).exists():
                print(f"âŒ Baselineæ–‡ä»¶ä¸å­˜åœ¨: {baseline_file}")
                return

            if not Path(finetuned_file).exists():
                print(f"âŒ å¾®è°ƒç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {finetuned_file}")
                print("   è¯·å…ˆè¿è¡Œè®­ç»ƒå’Œè¯„ä¼°")
                return

            compare_results(baseline_file, finetuned_file)
        else:
            # æŸ¥çœ‹æŒ‡å®šæ–‡ä»¶
            if Path(arg).exists():
                results = load_results(arg)
                print_metrics(results)
            else:
                print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {arg}")


if __name__ == "__main__":
    main()
