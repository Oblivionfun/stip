#!/usr/bin/env python3
"""
æ˜¾ç¤ºè¯„ä¼°çš„è¯¦ç»†ç»“æžœï¼ŒåŒ…æ‹¬thinkingè¿‡ç¨‹å’Œå†³ç­–ç»Ÿè®¡
"""

import json
import sys
from pathlib import Path
from collections import Counter


def load_results(filepath):
    """åŠ è½½è¯„ä¼°ç»“æžœ"""
    with open(filepath, 'r', encoding='utf-8') as f:
        return json.load(f)


def analyze_predictions(results):
    """åˆ†æžé¢„æµ‹ç»“æžœ"""

    predictions = results.get('predictions', [])
    ground_truth = results.get('ground_truth', [])

    if not predictions:
        print("âŒ è¯„ä¼°ç»“æžœä¸­æ²¡æœ‰ä¿å­˜predictions")
        return

    # ç»Ÿè®¡å†³ç­–åˆ†å¸ƒ
    pred_plans = []
    gt_plans = []

    for pred in predictions:
        if pred:
            plan = pred.get('plan', '')
            pred_plans.append('æ”¹é“' if 'update path' in plan.lower() else 'ä¸æ”¹é“')

    for gt in ground_truth:
        if gt and 'response' in gt:
            plan = gt['response'].get('plan', '')
            gt_plans.append('æ”¹é“' if 'update path' in plan.lower() else 'ä¸æ”¹é“')

    print("\n" + "=" * 80)
    print("å†³ç­–åˆ†å¸ƒç»Ÿè®¡")
    print("=" * 80)

    print(f"\nðŸ“Š é¢„æµ‹å†³ç­–åˆ†å¸ƒ (å‰{len(pred_plans)}ä¸ªæ ·æœ¬):")
    pred_counter = Counter(pred_plans)
    for decision, count in pred_counter.items():
        pct = count / len(pred_plans) * 100 if pred_plans else 0
        print(f"  â€¢ {decision}: {count} ({pct:.1f}%)")

    print(f"\nâœ… Ground Truthå†³ç­–åˆ†å¸ƒ (å‰{len(gt_plans)}ä¸ªæ ·æœ¬):")
    gt_counter = Counter(gt_plans)
    for decision, count in gt_counter.items():
        pct = count / len(gt_plans) * 100 if gt_plans else 0
        print(f"  â€¢ {decision}: {count} ({pct:.1f}%)")

    # æ˜¾ç¤ºthinkingç¤ºä¾‹
    print("\n" + "=" * 80)
    print("Thinking è¿‡ç¨‹ç¤ºä¾‹ (å‰3ä¸ª)")
    print("=" * 80)

    valid_preds = [p for p in predictions if p is not None]

    for i, (pred, gt) in enumerate(zip(valid_preds[:3], ground_truth[:3]), 1):
        print(f"\n{'='*80}")
        print(f"æ ·æœ¬ {i}: {gt.get('id', 'N/A')}")
        print(f"{'='*80}")

        # Ground Truth
        gt_resp = gt.get('response', {})
        gt_thinking = gt_resp.get('thinking', 'N/A')
        gt_reflection = gt_resp.get('reflection', 'N/A')
        gt_plan = gt_resp.get('plan', 'N/A')

        print(f"\nâœ… Ground Truth:")
        print(f"   Thinking (å‰150å­—ç¬¦):")
        print(f"   {gt_thinking[:150]}...")
        print(f"\n   Reflection (å‰100å­—ç¬¦):")
        print(f"   {gt_reflection[:100]}...")
        print(f"\n   Plan: {gt_plan}")

        # Prediction
        pred_thinking = pred.get('thinking', 'N/A')
        pred_reflection = pred.get('reflection', 'N/A')
        pred_plan = pred.get('plan', 'N/A')

        print(f"\nðŸ¤– æ¨¡åž‹é¢„æµ‹:")
        print(f"   Thinking (å‰150å­—ç¬¦):")
        print(f"   {pred_thinking[:150]}...")
        print(f"\n   Reflection (å‰100å­—ç¬¦):")
        print(f"   {pred_reflection[:100]}...")
        print(f"\n   Plan: {pred_plan}")

        # å¯¹æ¯”
        match = 'âœ… åŒ¹é…' if gt_plan == pred_plan else 'âŒ ä¸åŒ¹é…'
        print(f"\n   å†³ç­–å¯¹æ¯”: {match}")

    print("\n" + "=" * 80)

    # è®¡ç®—thinkingå­—æ®µçš„è¦†ç›–çŽ‡
    pred_with_thinking = sum(1 for p in predictions if p and p.get('thinking'))
    total_pred = len([p for p in predictions if p])

    print(f"\nðŸ“Š Thinkingå­—æ®µç»Ÿè®¡:")
    print(f"  â€¢ åŒ…å«thinkingçš„é¢„æµ‹: {pred_with_thinking}/{total_pred}")
    if total_pred > 0:
        print(f"  â€¢ è¦†ç›–çŽ‡: {pred_with_thinking/total_pred*100:.1f}%")


def main():
    if len(sys.argv) < 2:
        result_file = "outputs/evaluation_results.json"
    else:
        result_file = sys.argv[1]

    if not Path(result_file).exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {result_file}")
        print("\nä½¿ç”¨æ–¹æ³•:")
        print(f"  python {sys.argv[0]} [ç»“æžœæ–‡ä»¶è·¯å¾„]")
        print(f"  é»˜è®¤: {result_file}")
        return

    print("=" * 80)
    print(f"åˆ†æžè¯„ä¼°ç»“æžœ: {result_file}")
    print("=" * 80)

    results = load_results(result_file)

    # æ˜¾ç¤ºæ•´ä½“æŒ‡æ ‡
    print(f"\nðŸ“Š æ•´ä½“æ€§èƒ½:")
    metrics = results.get('decision_metrics', {})
    print(f"  â€¢ è·¯å¾„é€‰æ‹©å‡†ç¡®çŽ‡: {metrics.get('route_selection_accuracy', 0)*100:.2f}%")
    print(f"  â€¢ æ”¹é“F1åˆ†æ•°: {metrics.get('reroute_f1', 0):.4f}")
    print(f"  â€¢ Perplexity: {results.get('perplexity', 0):.2f}")
    print(f"  â€¢ æ€»æ ·æœ¬æ•°: {results.get('num_samples', 0)}")

    # åˆ†æžpredictions
    analyze_predictions(results)

    print("\nâœ“ åˆ†æžå®Œæˆ")


if __name__ == "__main__":
    main()
