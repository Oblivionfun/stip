# å®Œæ•´è¿è¡ŒæŒ‡å—

> **ç›®æ ‡**: ä»é—®å·æ•°æ®åˆ°å¯ç”¨çš„è·¯å¾„å†³ç­–æ™ºèƒ½ä½“
> **é¢„è®¡æ€»æ—¶é—´**: æ•°æ®å¤„ç†15ç§’ + æ¨¡å‹è®­ç»ƒæ•°å°æ—¶

---

## ğŸ“‹ å‰ç½®å‡†å¤‡

### 1. ç¯å¢ƒè¦æ±‚

**ç¡¬ä»¶**ï¼š
- âœ… GPU: è‡³å°‘16GBæ˜¾å­˜ï¼ˆæ¨è32GBï¼‰
- âœ… å†…å­˜: è‡³å°‘32GB RAM
- âœ… å­˜å‚¨: è‡³å°‘50GBå¯ç”¨ç©ºé—´

**è½¯ä»¶**ï¼š
- Python 3.8+
- CUDA 11.7+ (ç”¨äºGPUè®­ç»ƒ)

### 2. æ£€æŸ¥æ•°æ®æ–‡ä»¶

ç¡®ä¿ä»¥ä¸‹æ•°æ®æ–‡ä»¶å­˜åœ¨ï¼š
```bash
ls -lh data/
# åº”è¯¥çœ‹åˆ°ï¼š
# CN_dataset.xlsx  (çº¦1-2MB)
# UK_dataset.xlsx  (å¯é€‰)
# US_dataset.xlsx  (å¯é€‰)
```

### 3. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶

ç¡®ä¿Qwen3æ¨¡å‹å·²ä¸‹è½½ï¼š
```bash
ls -lh model/models/
# åº”è¯¥çœ‹åˆ°ï¼š
# config.json
# model-00001-of-00005.safetensors (~3.8GB)
# model-00002-of-00005.safetensors (~3.8GB)
# ... (æ€»è®¡çº¦15-16GB)
# tokenizer.json
```

---

## ğŸš€ å®Œæ•´è¿è¡Œæµç¨‹

### æ­¥éª¤1ï¼šå®‰è£…ä¾èµ– (çº¦2-5åˆ†é’Ÿ)

```bash
# åŸºç¡€ä¾èµ–
pip install pandas numpy scikit-learn matplotlib seaborn pyyaml openpyxl scipy

# æ·±åº¦å­¦ä¹ ä¾èµ–
pip install torch transformers accelerate datasets peft bitsandbytes

# å¯é€‰ï¼šå®‰è£…unslothåŠ é€Ÿè®­ç»ƒï¼ˆéœ€è¦ç‰¹å®šGPUå’ŒCUDAç‰ˆæœ¬ï¼‰
# pip install unsloth
```

**éªŒè¯å®‰è£…**ï¼š
```bash
python -c "import torch; print('PyTorch:', torch.__version__); print('CUDA available:', torch.cuda.is_available())"
```

---

### æ­¥éª¤2ï¼šPersonaå»ºæ¨¡ (çº¦10ç§’)

**åŠŸèƒ½**: ä»é—®å·æ•°æ®ç”Ÿæˆ1002ä¸ªé©¾é©¶å‘˜Persona

**è¿è¡Œ**ï¼š
```bash
python run_persona_modeling.py
```

**è¾“å‡ºæ–‡ä»¶**ï¼š
```
outputs/
â”œâ”€â”€ cleaned_survey_data.csv      (299KB)   - æ¸…æ´—åçš„é—®å·æ•°æ®
â”œâ”€â”€ preference_factors.csv       (166KB)   - 8ä¸ªåå¥½å› å­å¾—åˆ†
â”œâ”€â”€ personas.json                (1.5MB)   - GATSimæ ¼å¼Personaå¯¹è±¡
â”œâ”€â”€ persona_types.json           (88KB)    - 6ç§é©¾é©¶å‘˜ç±»å‹æ ‡ç­¾
â”œâ”€â”€ factor_loadings_heatmap.png  (135KB)   - å› å­è½½è·å¯è§†åŒ–
â””â”€â”€ persona_clustering.png       (435KB)   - èšç±»å¯è§†åŒ–
```

**éªŒè¯**ï¼š
```bash
# æ£€æŸ¥ç”Ÿæˆçš„Personaæ•°é‡
python -c "import json; personas=json.load(open('outputs/personas.json')); print(f'Generated {len(personas)} personas')"

# æŸ¥çœ‹ä¸€ä¸ªPersonaç¤ºä¾‹
head -50 outputs/personas.json
```

**é¢„æœŸç»“æœ**ï¼š
- âœ… 1002ä¸ªPersonaå¯¹è±¡
- âœ… 6ç§é©¾é©¶å‘˜ç±»å‹ï¼ˆè°¨æ…å‹ã€æ—¶é—´æ•æ„Ÿå‹ç­‰ï¼‰
- âœ… æ¯ä¸ªPersonaåŒ…å«8ä¸ªåå¥½å› å­

---

### æ­¥éª¤3ï¼šè®­ç»ƒæ•°æ®æ„é€  (çº¦5ç§’)

**åŠŸèƒ½**: ä¸ºæ¯ä¸ªPersonaç”Ÿæˆ10ä¸ªåœºæ™¯ï¼Œæ„é€ 10,000ä¸ªè®­ç»ƒæ ·æœ¬

**è¿è¡Œ**ï¼š
```bash
python run_data_construction.py
```

**è¾“å‡ºæ–‡ä»¶**ï¼š
```
outputs/
â”œâ”€â”€ scenarios.json            (~20MB)  - 10,020ä¸ªè·¯å¾„é€‰æ‹©åœºæ™¯
â”œâ”€â”€ decisions.json            (~3MB)   - 10,020ä¸ªå†³ç­–ç»“æœ
â”œâ”€â”€ train_samples.jsonl       (15.5MB) - 9,018ä¸ªè®­ç»ƒæ ·æœ¬
â”œâ”€â”€ validation_samples.jsonl  (1.7MB)  - 1,002ä¸ªéªŒè¯æ ·æœ¬
â””â”€â”€ *_statistics.json                  - ç»Ÿè®¡ä¿¡æ¯
```

**éªŒè¯**ï¼š
```bash
# æ£€æŸ¥æ ·æœ¬æ•°é‡
wc -l outputs/train_samples.jsonl outputs/validation_samples.jsonl

# æŸ¥çœ‹ä¸€ä¸ªè®­ç»ƒæ ·æœ¬
head -1 outputs/train_samples.jsonl | python -m json.tool
```

**æ ·æœ¬æ ¼å¼ç¤ºä¾‹**ï¼š
```json
{
  "id": "CN_0001_S01",
  "prompt": "You play the role of the person:\nName: User_0001 | Age: 35...\nTime period: morning rush hour\n...",
  "response": {
    "thinking": "Let me analyze the current situation:\n- Route A (usual): 35min (delay: 10min)\n- Route B (alternative): 27min (delay: 0min)\n...",
    "reflection": "Based on my analysis, Route B is faster and more predictable...",
    "plan": "update path: Side_St, Local_Road_2",
    "concepts": []
  }
}
```

**é¢„æœŸç»“æœ**ï¼š
- âœ… 9,018ä¸ªè®­ç»ƒæ ·æœ¬ + 1,002ä¸ªéªŒè¯æ ·æœ¬
- âœ… æ”¹é“ç‡çº¦68%
- âœ… åŒ…å«thinkingï¼ˆæ€è€ƒè¿‡ç¨‹ï¼‰ã€reflectionï¼ˆæ¨ç†ï¼‰ã€planï¼ˆå†³ç­–ï¼‰

---

### æ­¥éª¤4ï¼šæ¨¡å‹è®­ç»ƒ (æ•°å°æ—¶ï¼Œå–å†³äºGPUæ€§èƒ½)

**åŠŸèƒ½**: ä½¿ç”¨LoRAå¾®è°ƒQwen3æ¨¡å‹

**é‡è¦æç¤º**ï¼š
- âš ï¸ è®­ç»ƒéœ€è¦æ•°å°æ—¶ï¼Œå»ºè®®åœ¨tmux/screenä¸­è¿è¡Œ
- âš ï¸ ç¡®ä¿GPUæ˜¾å­˜è¶³å¤Ÿï¼ˆè‡³å°‘16GBï¼Œæ¨è32GBï¼‰
- âš ï¸ è®­ç»ƒè¿‡ç¨‹ä¼šå ç”¨GPUï¼Œç¡®ä¿æ²¡æœ‰å…¶ä»–ä»»åŠ¡

**è¿è¡Œ**ï¼š
```bash
# æ–¹å¼1ï¼šç›´æ¥è¿è¡Œ
python run_model_training.py

# æ–¹å¼2ï¼šåœ¨tmuxä¸­è¿è¡Œï¼ˆæ¨èï¼‰
tmux new -s training
python run_model_training.py
# æŒ‰Ctrl+Bç„¶åæŒ‰D detach
# ç¨åç”¨ tmux attach -t training é‡è¿

# æ–¹å¼3ï¼šåå°è¿è¡Œ
nohup python run_model_training.py > training.log 2>&1 &
# æŸ¥çœ‹æ—¥å¿—ï¼štail -f training.log
```

**ç›‘æ§GPUä½¿ç”¨**ï¼ˆå¦å¼€ä¸€ä¸ªç»ˆç«¯ï¼‰ï¼š
```bash
watch -n 1 nvidia-smi
```

**è®­ç»ƒé…ç½®** (åœ¨ `configs/training_config.yaml` ä¸­)ï¼š
- Epochs: 3
- Batch size: 2 Ã— 4 (æ¢¯åº¦ç´¯ç§¯) = 8æœ‰æ•ˆbatch
- LoRA rank: 16
- Learning rate: 2e-4
- ä¼˜åŒ–å™¨: AdamW 8bit
- é‡åŒ–: 4bit

**è¾“å‡ºæ–‡ä»¶**ï¼š
```
checkpoints/sft_model/
â”œâ”€â”€ adapter_model.safetensors  (~100MB)  - LoRAæƒé‡
â”œâ”€â”€ adapter_config.json
â”œâ”€â”€ tokenizer.json
â”œâ”€â”€ config.json
â””â”€â”€ training_args.bin
```

**è®­ç»ƒæ—¥å¿—**ï¼š
```
outputs/logs/sft_trainer.log
```

**é¢„æœŸè®­ç»ƒæ—¶é—´**ï¼š
- A100: 2-3å°æ—¶
- V100: 4-6å°æ—¶
- RTX 3090: 3-5å°æ—¶
- RTX 4090: 2-4å°æ—¶

**éªŒè¯**ï¼š
```bash
# æ£€æŸ¥checkpointæ˜¯å¦ç”Ÿæˆ
ls -lh checkpoints/sft_model/

# æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
tail -100 outputs/logs/sft_trainer.log
```

---

### æ­¥éª¤5ï¼šæ¨¡å‹è¯„ä¼° (çº¦10-30åˆ†é’Ÿ)

**åŠŸèƒ½**: è¯„ä¼°æ¨¡å‹çš„å†³ç­–å‡†ç¡®ç‡ã€æ”¹é“F1ã€Perplexity

**è¿è¡Œ**ï¼š
```bash
python src/training/evaluator.py
```

**è¯„ä¼°æŒ‡æ ‡**ï¼š
1. **è·¯å¾„é€‰æ‹©å‡†ç¡®ç‡** - ç›®æ ‡â‰¥75%
2. **æ”¹é“å†³ç­–F1åˆ†æ•°** - ç›®æ ‡â‰¥0.80
3. **Perplexity** - è¶Šä½è¶Šå¥½ï¼ˆ<10ä¸ºä¼˜ç§€ï¼‰

**è¾“å‡º**ï¼š
```
================================================================================
æ¨¡å‹è¯„ä¼°ç»“æœ
================================================================================

å†³ç­–å‡†ç¡®ç‡æŒ‡æ ‡:
  - è·¯å¾„é€‰æ‹©å‡†ç¡®ç‡: 78.50%
  - æ”¹é“å†³ç­–F1åˆ†æ•°: 0.8234
    - Precision: 0.8456
    - Recall: 0.8021

è¯­è¨€æ¨¡å‹æŒ‡æ ‡:
  - Perplexity: 4.23

è¯¦ç»†ç»Ÿè®¡:
  - æ€»æ ·æœ¬æ•°: 1002
  - æ­£ç¡®è·¯å¾„é€‰æ‹©: 787
  - æ”¹é“TP: 552
  - æ”¹é“FP: 101
  - æ”¹é“FN: 136
================================================================================
```

**ç»“æœæ–‡ä»¶**ï¼š
```
outputs/evaluation_results.json
```

---

## ğŸ“Š è¿è¡Œè¿›åº¦è¿½è¸ª

### å¿«é€Ÿæ£€æŸ¥è¿›åº¦

```bash
# æ£€æŸ¥æ¯ä¸ªæ­¥éª¤çš„è¾“å‡ºæ–‡ä»¶
ls -lh outputs/personas.json              # æ­¥éª¤2å®Œæˆ
ls -lh outputs/train_samples.jsonl       # æ­¥éª¤3å®Œæˆ
ls -lh checkpoints/sft_model/             # æ­¥éª¤4å®Œæˆ
ls -lh outputs/evaluation_results.json   # æ­¥éª¤5å®Œæˆ
```

### è¯¦ç»†æ—¥å¿—ä½ç½®

```bash
outputs/logs/
â”œâ”€â”€ data_loader.log           - æ•°æ®åŠ è½½æ—¥å¿—
â”œâ”€â”€ factor_analysis.log       - å› å­åˆ†ææ—¥å¿—
â”œâ”€â”€ persona_generator.log     - Personaç”Ÿæˆæ—¥å¿—
â”œâ”€â”€ persona_clustering.log    - èšç±»æ—¥å¿—
â”œâ”€â”€ scenario_generator.log    - åœºæ™¯ç”Ÿæˆæ—¥å¿—
â”œâ”€â”€ decision_simulator.log    - å†³ç­–æ¨¡æ‹Ÿæ—¥å¿—
â”œâ”€â”€ sample_builder.log        - æ ·æœ¬æ„é€ æ—¥å¿—
â”œâ”€â”€ sft_trainer.log           - æ¨¡å‹è®­ç»ƒæ—¥å¿—ï¼ˆæœ€é‡è¦ï¼‰
â””â”€â”€ evaluator.log             - è¯„ä¼°æ—¥å¿—
```

---

## ğŸ› å¸¸è§é—®é¢˜æ’æŸ¥

### é—®é¢˜1ï¼šæ˜¾å­˜ä¸è¶³ (OOM)

**ç—‡çŠ¶**ï¼š
```
RuntimeError: CUDA out of memory
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```yaml
# ä¿®æ”¹ configs/training_config.yaml

# æ–¹æ¡ˆAï¼šå‡å°batch size
training:
  per_device_train_batch_size: 1  # ä»2æ”¹ä¸º1
  gradient_accumulation_steps: 8  # ä»4æ”¹ä¸º8

# æ–¹æ¡ˆBï¼šå‡å°åºåˆ—é•¿åº¦
model:
  max_seq_length: 1536  # ä»2048æ”¹ä¸º1536

# æ–¹æ¡ˆCï¼šå‡å°LoRA rank
lora:
  r: 8  # ä»16æ”¹ä¸º8
```

---

### é—®é¢˜2ï¼šè®­ç»ƒé€Ÿåº¦å¾ˆæ…¢

**æ£€æŸ¥GPUä½¿ç”¨**ï¼š
```bash
nvidia-smi
# GPUåˆ©ç”¨ç‡åº”è¯¥>80%
```

**å¯èƒ½åŸå› **ï¼š
- DataLoader workersä¸è¶³
- Batch sizeå¤ªå°
- æœªå¯ç”¨æ··åˆç²¾åº¦

**è§£å†³æ–¹æ¡ˆ**ï¼š
```yaml
# configs/training_config.yaml
training:
  dataloader_num_workers: 4  # å¢åŠ workers
  bf16: true                 # å¯ç”¨æ··åˆç²¾åº¦
```

---

### é—®é¢˜3ï¼šæ¨¡å‹ä¸æ”¶æ•›

**ç—‡çŠ¶**ï¼š
- è®­ç»ƒlossä¸ä¸‹é™
- éªŒè¯lossæŒç»­ä¸Šå‡
- è¯„ä¼°å‡†ç¡®ç‡å¾ˆä½

**æ£€æŸ¥**ï¼š
```bash
# æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
grep "loss" outputs/logs/sft_trainer.log

# æ£€æŸ¥å­¦ä¹ ç‡
grep "learning_rate" outputs/logs/sft_trainer.log
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. é™ä½å­¦ä¹ ç‡ï¼š`2e-4 -> 1e-4`
2. å¢åŠ warmupï¼š`100 -> 200`
3. æ£€æŸ¥æ•°æ®è´¨é‡ï¼šç¡®è®¤training dataæ ¼å¼æ­£ç¡®

---

### é—®é¢˜4ï¼šè¯„ä¼°ç»“æœä¸ç†æƒ³

**è·¯å¾„é€‰æ‹©å‡†ç¡®ç‡<70%**ï¼š
- æ£€æŸ¥è®­ç»ƒæ•°æ®è´¨é‡
- å¢åŠ è®­ç»ƒepochs
- è°ƒæ•´LoRAå‚æ•°

**æ”¹é“F1åˆ†æ•°<0.70**ï¼š
- æ•°æ®ä¸å¹³è¡¡ï¼ˆæ”¹é“vsä¸æ”¹é“ï¼‰
- å¢åŠ æ•°æ®å¤šæ ·æ€§
- è°ƒæ•´å†³ç­–é˜ˆå€¼

---

## ğŸ¯ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. æ•°æ®é¢„å¤„ç†

**è·³è¿‡å·²å®Œæˆçš„æ­¥éª¤**ï¼š
```bash
# å¦‚æœpersonas.jsonå·²å­˜åœ¨ï¼Œç›´æ¥è·³åˆ°æ•°æ®æ„é€ 
python run_data_construction.py

# å¦‚æœtrain_samples.jsonlå·²å­˜åœ¨ï¼Œç›´æ¥å¼€å§‹è®­ç»ƒ
python run_model_training.py
```

---

### 2. è®­ç»ƒåŠ é€Ÿ

**ä½¿ç”¨æ›´å°‘çš„éªŒè¯æ ·æœ¬**ï¼š
```python
# src/training/evaluator.py
results = evaluator.run_evaluation(num_samples=100)  # ä»…è¯„ä¼°100ä¸ªæ ·æœ¬
```

**å‡å°‘è¯„ä¼°é¢‘ç‡**ï¼š
```yaml
# configs/training_config.yaml
training:
  eval_steps: 1000  # ä»500æ”¹ä¸º1000
```

---

### 3. å¿«é€Ÿæµ‹è¯•

**ä½¿ç”¨æ›´å°‘çš„æ•°æ®å¿«é€ŸéªŒè¯æµç¨‹**ï¼š
```yaml
# configs/training_config.yaml
data:
  max_samples: 1000  # ä»…ç”¨1000ä¸ªæ ·æœ¬è®­ç»ƒ

training:
  num_train_epochs: 1  # ä»…è®­ç»ƒ1ä¸ªepoch
```

---

## ğŸ“ æ–‡ä»¶ç»“æ„æ€»è§ˆ

```
/root/stip/
â”œâ”€â”€ run_persona_modeling.py      # ğŸ”µ æ­¥éª¤2ï¼šPersonaå»ºæ¨¡
â”œâ”€â”€ run_data_construction.py     # ğŸ”µ æ­¥éª¤3ï¼šè®­ç»ƒæ•°æ®æ„é€ 
â”œâ”€â”€ run_model_training.py        # ğŸ”µ æ­¥éª¤4ï¼šæ¨¡å‹è®­ç»ƒ
â”‚
â”œâ”€â”€ data/                         # åŸå§‹é—®å·æ•°æ®
â”‚   â””â”€â”€ CN_dataset.xlsx
â”‚
â”œâ”€â”€ model/models/                 # Qwen3æ¨¡å‹
â”‚   â”œâ”€â”€ config.json
â”‚   â””â”€â”€ model-*.safetensors
â”‚
â”œâ”€â”€ configs/                      # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ preference_config.yaml   - Personaå»ºæ¨¡é…ç½®
â”‚   â”œâ”€â”€ scenario_config.yaml     - åœºæ™¯ç”Ÿæˆé…ç½®
â”‚   â””â”€â”€ training_config.yaml     - è®­ç»ƒé…ç½®
â”‚
â”œâ”€â”€ src/                          # æºä»£ç 
â”‚   â”œâ”€â”€ preference_modeling/     - Personaå»ºæ¨¡æ¨¡å—
â”‚   â”œâ”€â”€ data_construction/       - æ•°æ®æ„é€ æ¨¡å—
â”‚   â”œâ”€â”€ training/                - è®­ç»ƒå’Œè¯„ä¼°æ¨¡å—
â”‚   â””â”€â”€ utils/                   - å·¥å…·å‡½æ•°
â”‚
â”œâ”€â”€ outputs/                      # æ‰€æœ‰è¾“å‡ºæ–‡ä»¶
â”‚   â”œâ”€â”€ personas.json            - Personaå¯¹è±¡
â”‚   â”œâ”€â”€ train_samples.jsonl      - è®­ç»ƒæ•°æ®
â”‚   â”œâ”€â”€ evaluation_results.json  - è¯„ä¼°ç»“æœ
â”‚   â””â”€â”€ logs/                    - æ—¥å¿—æ–‡ä»¶
â”‚
â”œâ”€â”€ checkpoints/                  # æ¨¡å‹æƒé‡
â”‚   â””â”€â”€ sft_model/               - å¾®è°ƒåçš„æ¨¡å‹
â”‚
â””â”€â”€ docs/                         # æ–‡æ¡£
    â”œâ”€â”€ QUICKSTART.md            - æœ¬æ–‡ä»¶
    â”œâ”€â”€ OPTIMIZATION_GUIDE.md    - ä¼˜åŒ–è¯´æ˜
    â””â”€â”€ THINKING_FEATURE.md      - ThinkingåŠŸèƒ½è¯´æ˜
```

---

## âœ… è¿è¡Œæ£€æŸ¥æ¸…å•

å®Œæˆæ¯ä¸ªæ­¥éª¤åï¼Œåœ¨å¯¹åº”çš„æ¡†ä¸­æ‰“å‹¾ï¼š

- [ ] **ç¯å¢ƒå‡†å¤‡**
  - [ ] Pythonä¾èµ–å·²å®‰è£…
  - [ ] GPUå¯ç”¨ä¸”æ˜¾å­˜è¶³å¤Ÿ
  - [ ] é—®å·æ•°æ®æ–‡ä»¶å­˜åœ¨
  - [ ] Qwen3æ¨¡å‹æ–‡ä»¶å­˜åœ¨

- [ ] **æ­¥éª¤2ï¼šPersonaå»ºæ¨¡** (çº¦10ç§’)
  - [ ] è¿è¡Œ `python run_persona_modeling.py`
  - [ ] ç”Ÿæˆäº† `outputs/personas.json` (1002ä¸ª)
  - [ ] ç”Ÿæˆäº†å¯è§†åŒ–å›¾è¡¨

- [ ] **æ­¥éª¤3ï¼šè®­ç»ƒæ•°æ®æ„é€ ** (çº¦5ç§’)
  - [ ] è¿è¡Œ `python run_data_construction.py`
  - [ ] ç”Ÿæˆäº† `outputs/train_samples.jsonl` (9018ä¸ª)
  - [ ] ç”Ÿæˆäº† `outputs/validation_samples.jsonl` (1002ä¸ª)
  - [ ] æ ·æœ¬æ ¼å¼åŒ…å«thinkingã€reflectionã€plan

- [ ] **æ­¥éª¤4ï¼šæ¨¡å‹è®­ç»ƒ** (æ•°å°æ—¶)
  - [ ] è¿è¡Œ `python run_model_training.py`
  - [ ] è®­ç»ƒlossç¨³å®šä¸‹é™
  - [ ] ç”Ÿæˆäº† `checkpoints/sft_model/`
  - [ ] åŒ…å«adapteræƒé‡å’Œé…ç½®æ–‡ä»¶

- [ ] **æ­¥éª¤5ï¼šæ¨¡å‹è¯„ä¼°** (çº¦10-30åˆ†é’Ÿ)
  - [ ] è¿è¡Œ `python src/training/evaluator.py`
  - [ ] è·¯å¾„é€‰æ‹©å‡†ç¡®ç‡ â‰¥ 70%
  - [ ] æ”¹é“F1åˆ†æ•° â‰¥ 0.70
  - [ ] ç”Ÿæˆäº† `outputs/evaluation_results.json`

---

## ğŸ“ ä¸‹ä¸€æ­¥

æ¨¡å‹è®­ç»ƒå®Œæˆåï¼Œå¯ä»¥ï¼š

1. **éƒ¨ç½²æ¨ç†æœåŠ¡**
   ```python
   from transformers import AutoTokenizer, AutoModelForCausalLM

   tokenizer = AutoTokenizer.from_pretrained("checkpoints/sft_model")
   model = AutoModelForCausalLM.from_pretrained("checkpoints/sft_model")

   # æ¨ç†
   prompt = "You play the role..."
   response = model.generate(...)
   ```

2. **é›†æˆåˆ°åº”ç”¨**
   - æ„å»ºAPIæœåŠ¡
   - æ¥å…¥å¯¼èˆªç³»ç»Ÿ
   - å®æ—¶è·¯å¾„å†³ç­–

3. **ç»§ç»­ä¼˜åŒ–**
   - æ”¶é›†çœŸå®åé¦ˆæ•°æ®
   - åœ¨çº¿å­¦ä¹ æ›´æ–°
   - A/Bæµ‹è¯•æ•ˆæœ

---

## ğŸ“ è·å–å¸®åŠ©

**æ—¥å¿—æŸ¥çœ‹**ï¼š
```bash
# æŸ¥çœ‹æœ€è¿‘çš„é”™è¯¯
tail -100 outputs/logs/sft_trainer.log | grep -i error

# å®æ—¶ç›‘æ§è®­ç»ƒ
tail -f outputs/logs/sft_trainer.log
```

**å¸¸ç”¨å‘½ä»¤**ï¼š
```bash
# æ£€æŸ¥GPUä½¿ç”¨
nvidia-smi

# æ£€æŸ¥ç£ç›˜ç©ºé—´
df -h

# æ£€æŸ¥è¿›ç¨‹
ps aux | grep python

# æ€æ­»è®­ç»ƒè¿›ç¨‹ï¼ˆè°¨æ…ä½¿ç”¨ï¼‰
pkill -f run_model_training
```

---

**ç¥æ‚¨è¿è¡Œé¡ºåˆ©ï¼å¦‚æœ‰é—®é¢˜è¯·æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶æˆ–æ–‡æ¡£** ğŸš€
